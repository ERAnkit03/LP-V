{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2054374,"sourceType":"datasetVersion","datasetId":1230956}],"dockerImageVersionId":30066,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 1. Input the required libraries","metadata":{"id":"Zc_YsJkfOX77"}},{"cell_type":"code","source":"import tensorflow as tf\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n","metadata":{"id":"g-KdSILKOzmK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 2. Load the dataset and normalize every image","metadata":{"id":"4Cf6BxCeSKVS"}},{"cell_type":"markdown","source":"## Load the dataset ","metadata":{}},{"cell_type":"code","source":"data=np.load('../input/face-image-database/ORL_faces.npz')\ndata\n","metadata":{"id":"KfsfF2quOz3K","outputId":"130f4bf2-4e86-4047-cc93-5f897e1da111","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_img= pd.DataFrame.from_dict({item: data[item] for item in data.files}, orient='index')\ndata_img","metadata":{"id":"M8_CBHpgOz5-","outputId":"e458afdd-1f91-4318-f780-827ce1adbfe8","scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Normalize every image in the dataset","metadata":{}},{"cell_type":"code","source":"# Normalizing training data\nX_train = np.array(data['trainX'],dtype='float32')/255\nX_train","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalizing testing data\nX_test = np.array(data['testX'],dtype='float32')/255\nX_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load image labels","metadata":{"id":"HpVY2qivRKO4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train= np.array(data['trainY'])\ny_test= np.array(data['testY'])","metadata":{"id":"cIkJfIEVRMwx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 3. Split the dataset","metadata":{"id":"mbDeEd2lRTYY"}},{"cell_type":"code","source":"print('Training dataset X_train, has', X_train.shape[0], 'rows and', X_train.shape[1], 'columns')\nprint('Testing dataset X_test, has', X_test.shape[0], 'rows and', X_test.shape[1], 'columns')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4. Transform the images to equal sizes to feed in CNN","metadata":{"id":"xO_e-Jk7RXjI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_rows=112\nimg_cols=92\nbatch_size=512\nimg_shape=(img_rows, img_cols, 1)","metadata":{"id":"vZ3YBuH9RdIx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## change the size of images","metadata":{"id":"_lfsLq6SRfLJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.dtype","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Training dataset X_train, has', X_train.shape[0], 'and', X_train.shape[1], 'columns')\nprint('Testing dataset X_test, has', X_test.shape[0], 'and', X_test.shape[1], 'columns')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reshaping X_train and X_test dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = X_train.reshape(X_train.shape[0], *img_shape)\nX_test = X_test.reshape(X_test.shape[0], *img_shape)","metadata":{"id":"XsO_48wBRhbp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('After reshaping, training dataset X_train, has the dimensions', X_train.shape)\nprint('After reshaping, testing dataset X_test, has the dimensions', X_test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Converting class vectors to binary class matrices for y_train and y_test","metadata":{}},{"cell_type":"code","source":"y_train = tf.keras.utils.to_categorical(y_train, 40)\ny_test = tf.keras.utils.to_categorical(y_test, 40)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('After converting class vectors to binary class matrices, label dataset y_train, has', y_train.shape[0], 'rows and', y_train.shape[1], 'columns')\nprint('After converting class vectors to binary class matrices, label dataset y_test, has', y_test.shape[0], 'rows and', y_test.shape[1], 'columns')","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 5. Build a CNN model that has 3 main layers:\n## i. Convolutional Layer\n## ii. Pooling Layer\n## iii. Fully Connected Layer","metadata":{}},{"cell_type":"code","source":"# Network training parameters\nEPOCHS = 50\nBATCH_SIZE = 512\nVERBOSE = 1\nOPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.001)\nVALIDATION_SPLIT = 0.95\nlr=0.001\n\nINPUT_SHAPE = img_shape\nNB_CLASSES = 40 # number of outputs = number of digits","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First convolution layer\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Conv2D(36, (5,5), activation='relu', input_shape = INPUT_SHAPE, name=\"ConvLay1\"))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n# Second convolution layer\nmodel.add(tf.keras.layers.Conv2D(54, (5,5), activation='relu', input_shape = INPUT_SHAPE, name=\"ConvLay2\"))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n# Flatten the layers\nmodel.add(tf.keras.layers.Flatten())\n\n# Feed to Dense network\nmodel.add(tf.keras.layers.Dense(units = 1024, activation = 'relu'))\nmodel.add(tf.keras.layers.Dense(units = 512, activation = 'relu'))\nmodel.add(tf.keras.layers.Dense(units = 256, activation = 'relu'))\n\n# a softmax classifier\nmodel.add(tf.keras.layers.Dense(units = NB_CLASSES, activation='softmax'))","metadata":{"id":"Kh1UpqG5RnzI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer=OPTIMIZER,metrics=['accuracy'], loss='categorical_crossentropy')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fit the model\nhistory = model.fit(np.array(X_train), np.array(y_train),\n                    batch_size = BATCH_SIZE,\n                    epochs = EPOCHS,\n                    verbose = VERBOSE,\n                    validation_data=(np.array(X_test),np.array(y_test)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 6. Train the model","metadata":{"id":"rjnM54SMRshZ"}},{"cell_type":"code","source":"score = model.evaluate(X_test, y_test, verbose=VERBOSE)\nprint('\\nTest loss:', score[0])\nprint('\\nTest accuracy:', score[1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 7. Plot the result","metadata":{"id":"cONIjCaeR2sw"}},{"cell_type":"code","source":"plt.title('Model accuracy')\nplt.plot(history.history['val_accuracy'])\nplt.plot(history.history['accuracy'])\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['test', 'train'], loc='lower right')\nplt.show()\n\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['test', 'train'], loc='upper right')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}